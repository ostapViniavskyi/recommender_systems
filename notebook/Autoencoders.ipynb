{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join('..', 'data', 'processed', 'sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings = load_npz(os.path.join(DATA_DIR, 'train_ratings.npz'))\n",
    "test_ratings = load_npz(os.path.join(DATA_DIR, 'test_ratings.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9166, 1998)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, N = train_ratings.shape\n",
    "M, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask of locations where sparse matrices have nonzero values\n",
    "train_mask = (train_ratings > 0).astype(np.uint8)\n",
    "test_mask = (test_ratings > 0).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2368585786572535"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute global mean\n",
    "mean = train_ratings.sum() / train_mask.sum()\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(M, N, K, lmbda=0.):\n",
    "    \"\"\"\n",
    "    Build a model for matrix factorization\n",
    "    Args:\n",
    "        M (int): number of users\n",
    "        N (int): number of items\n",
    "        K (int): size of hidden layer\n",
    "        lmbda (float): L2-regularization parameter\n",
    "    Returns:\n",
    "        keras.models.Model: model for MF\n",
    "    \"\"\"\n",
    "    user_vector = Input((N, )) \n",
    "    \n",
    "    noisy_user_vector = Dropout(rate=0.8)(user_vector)\n",
    "    hidden = Dense(K, activation='relu', kernel_regularizer=l2(lmbda))(noisy_user_vector)\n",
    "    output = Dense(N, kernel_regularizer=l2(lmbda))(hidden)\n",
    "    \n",
    "    model = Model(user_vector, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of keras-model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(M, N, K=900, lmbda=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1998)]            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1998)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 900)               1799100   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1998)              1800198   \n",
      "=================================================================\n",
      "Total params: 3,599,298\n",
      "Trainable params: 3,599,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mse(y_true, y_pred):\n",
    "    true_mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
    "    sq_residuals = ((y_pred - y_true) ** 2) * true_mask # include only residual from non zero inputs\n",
    "    return tf.reduce_sum(sq_residuals) / tf.reduce_sum(true_mask)\n",
    "\n",
    "def sparse_rmse(y_true, y_pred):\n",
    "    return tf.sqrt(sparse_mse(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_datagen(train_ratings, train_mask, batch_size=128):\n",
    "    while True:\n",
    "        train_ratings, train_mask = shuffle(train_ratings, train_mask)\n",
    "        steps = train_ratings.shape[0] // batch_size\n",
    "        for step in range(steps):\n",
    "            train_batch = train_ratings[step * batch_size:(step + 1) * batch_size].toarray()\n",
    "            mask_batch = train_mask[step * batch_size:(step + 1) * batch_size].toarray()  \n",
    "            # normilize nonzero values\n",
    "            train_batch = train_batch - mean * mask_batch\n",
    "            yield train_batch, train_batch\n",
    "\n",
    "def test_datagen(train_ratings, train_mask, test_ratings, test_mask, batch_size=128):\n",
    "    while True:\n",
    "        steps = train_ratings.shape[0] // batch_size\n",
    "        for step in range(steps):\n",
    "            train_batch = train_ratings[step * batch_size:(step + 1) * batch_size].toarray()\n",
    "            mask_batch = train_mask[step * batch_size:(step + 1) * batch_size].toarray() \n",
    "            test_batch = test_ratings[step * batch_size:(step + 1) * batch_size].toarray()\n",
    "            test_mask_batch = test_mask[step * batch_size:(step + 1) * batch_size].toarray() \n",
    "            # normilize nonzero values\n",
    "            train_batch = train_batch - mean * mask_batch\n",
    "            test_batch = test_batch - mean * test_mask_batch\n",
    "            \n",
    "            yield train_batch, test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS = sparse_mse\n",
    "LR = 0.1\n",
    "RATE_DECAY = 0\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=LOSS,\n",
    "    optimizer=SGD(lr=LR, decay=RATE_DECAY, momentum=0.9),\n",
    "    metrics=[sparse_rmse]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 4s 107ms/step - loss: 3.3584 - sparse_rmse: 0.9879 - val_loss: 3.1887 - val_sparse_rmse: 0.9734\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 2.9766 - sparse_rmse: 0.9401 - val_loss: 2.8814 - val_sparse_rmse: 0.9626\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 2.6626 - sparse_rmse: 0.9150 - val_loss: 2.6188 - val_sparse_rmse: 0.9550\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 2.3978 - sparse_rmse: 0.8964 - val_loss: 2.3924 - val_sparse_rmse: 0.9482\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 2.1790 - sparse_rmse: 0.8854 - val_loss: 2.1964 - val_sparse_rmse: 0.9420\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 1.9906 - sparse_rmse: 0.8761 - val_loss: 2.0261 - val_sparse_rmse: 0.9362\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 1.8236 - sparse_rmse: 0.8653 - val_loss: 1.8799 - val_sparse_rmse: 0.9315\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 4s 108ms/step - loss: 1.6826 - sparse_rmse: 0.8582 - val_loss: 1.7537 - val_sparse_rmse: 0.9276\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 1.5651 - sparse_rmse: 0.8544 - val_loss: 1.6447 - val_sparse_rmse: 0.9245\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 1.4617 - sparse_rmse: 0.8503 - val_loss: 1.5502 - val_sparse_rmse: 0.9216\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 1.3744 - sparse_rmse: 0.8482 - val_loss: 1.4694 - val_sparse_rmse: 0.9198\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 1.2943 - sparse_rmse: 0.8433 - val_loss: 1.3998 - val_sparse_rmse: 0.9182\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 1.2308 - sparse_rmse: 0.8427 - val_loss: 1.3395 - val_sparse_rmse: 0.9171\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 1.1746 - sparse_rmse: 0.8415 - val_loss: 1.2866 - val_sparse_rmse: 0.9154\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 1.1249 - sparse_rmse: 0.8399 - val_loss: 1.2412 - val_sparse_rmse: 0.9146\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 1.0827 - sparse_rmse: 0.8391 - val_loss: 1.2017 - val_sparse_rmse: 0.9136\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 1.0463 - sparse_rmse: 0.8382 - val_loss: 1.1671 - val_sparse_rmse: 0.9124\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 1.0125 - sparse_rmse: 0.8362 - val_loss: 1.1377 - val_sparse_rmse: 0.9118\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.9834 - sparse_rmse: 0.8343 - val_loss: 1.1120 - val_sparse_rmse: 0.9111\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 0.9657 - sparse_rmse: 0.8371 - val_loss: 1.0898 - val_sparse_rmse: 0.9101\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 0.9449 - sparse_rmse: 0.8365 - val_loss: 1.0700 - val_sparse_rmse: 0.9092\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.9253 - sparse_rmse: 0.8349 - val_loss: 1.0539 - val_sparse_rmse: 0.9092\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 0.9134 - sparse_rmse: 0.8365 - val_loss: 1.0386 - val_sparse_rmse: 0.9084\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 0.8956 - sparse_rmse: 0.8337 - val_loss: 1.0249 - val_sparse_rmse: 0.9073\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.8845 - sparse_rmse: 0.8337 - val_loss: 1.0142 - val_sparse_rmse: 0.9071\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 0.8757 - sparse_rmse: 0.8344 - val_loss: 1.0033 - val_sparse_rmse: 0.9062\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 0.8642 - sparse_rmse: 0.8325 - val_loss: 0.9958 - val_sparse_rmse: 0.9062\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 0.8602 - sparse_rmse: 0.8342 - val_loss: 0.9882 - val_sparse_rmse: 0.9059\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.8574 - sparse_rmse: 0.8366 - val_loss: 0.9818 - val_sparse_rmse: 0.9059\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 0.8467 - sparse_rmse: 0.8339 - val_loss: 0.9762 - val_sparse_rmse: 0.9059\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 0.8452 - sparse_rmse: 0.8362 - val_loss: 0.9689 - val_sparse_rmse: 0.9046\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 0.8425 - sparse_rmse: 0.8373 - val_loss: 0.9648 - val_sparse_rmse: 0.9047\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 0.8333 - sparse_rmse: 0.8342 - val_loss: 0.9609 - val_sparse_rmse: 0.9047\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.8346 - sparse_rmse: 0.8371 - val_loss: 0.9577 - val_sparse_rmse: 0.9045\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.8308 - sparse_rmse: 0.8362 - val_loss: 0.9540 - val_sparse_rmse: 0.9040\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.8211 - sparse_rmse: 0.8319 - val_loss: 0.9522 - val_sparse_rmse: 0.9040\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.8264 - sparse_rmse: 0.8363 - val_loss: 0.9484 - val_sparse_rmse: 0.9030\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 4s 112ms/step - loss: 0.8223 - sparse_rmse: 0.8352 - val_loss: 0.9478 - val_sparse_rmse: 0.9037\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.8198 - sparse_rmse: 0.8346 - val_loss: 0.9426 - val_sparse_rmse: 0.9019\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.8210 - sparse_rmse: 0.8362 - val_loss: 0.9432 - val_sparse_rmse: 0.9030\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.8107 - sparse_rmse: 0.8311 - val_loss: 0.9411 - val_sparse_rmse: 0.9027\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 0.8131 - sparse_rmse: 0.8333 - val_loss: 0.9406 - val_sparse_rmse: 0.9028\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.8171 - sparse_rmse: 0.8364 - val_loss: 0.9383 - val_sparse_rmse: 0.9025\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 4s 112ms/step - loss: 0.8117 - sparse_rmse: 0.8338 - val_loss: 0.9390 - val_sparse_rmse: 0.9031\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 4s 110ms/step - loss: 0.8124 - sparse_rmse: 0.8347 - val_loss: 0.9358 - val_sparse_rmse: 0.9017\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8136 - sparse_rmse: 0.8355 - val_loss: 0.9370 - val_sparse_rmse: 0.9027\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8098 - sparse_rmse: 0.8336 - val_loss: 0.9351 - val_sparse_rmse: 0.9019\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8093 - sparse_rmse: 0.8334 - val_loss: 0.9344 - val_sparse_rmse: 0.9016\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8089 - sparse_rmse: 0.8334 - val_loss: 0.9341 - val_sparse_rmse: 0.9016\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8094 - sparse_rmse: 0.8339 - val_loss: 0.9334 - val_sparse_rmse: 0.9017\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8096 - sparse_rmse: 0.8345 - val_loss: 0.9326 - val_sparse_rmse: 0.9013\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8086 - sparse_rmse: 0.8340 - val_loss: 0.9317 - val_sparse_rmse: 0.9010\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8048 - sparse_rmse: 0.8320 - val_loss: 0.9321 - val_sparse_rmse: 0.9015\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8073 - sparse_rmse: 0.8336 - val_loss: 0.9313 - val_sparse_rmse: 0.9011\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8082 - sparse_rmse: 0.8343 - val_loss: 0.9316 - val_sparse_rmse: 0.9014\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8100 - sparse_rmse: 0.8353 - val_loss: 0.9315 - val_sparse_rmse: 0.9010\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8060 - sparse_rmse: 0.8329 - val_loss: 0.9313 - val_sparse_rmse: 0.9013\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8055 - sparse_rmse: 0.8327 - val_loss: 0.9314 - val_sparse_rmse: 0.9014\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8001 - sparse_rmse: 0.8295 - val_loss: 0.9309 - val_sparse_rmse: 0.9013\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8051 - sparse_rmse: 0.8328 - val_loss: 0.9293 - val_sparse_rmse: 0.9006\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8045 - sparse_rmse: 0.8327 - val_loss: 0.9306 - val_sparse_rmse: 0.9014\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 4s 112ms/step - loss: 0.8068 - sparse_rmse: 0.8339 - val_loss: 0.9294 - val_sparse_rmse: 0.9007\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8023 - sparse_rmse: 0.8313 - val_loss: 0.9290 - val_sparse_rmse: 0.9004\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8008 - sparse_rmse: 0.8304 - val_loss: 0.9311 - val_sparse_rmse: 0.9015\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8008 - sparse_rmse: 0.8305 - val_loss: 0.9291 - val_sparse_rmse: 0.9006\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8008 - sparse_rmse: 0.8301 - val_loss: 0.9292 - val_sparse_rmse: 0.9006\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8004 - sparse_rmse: 0.8302 - val_loss: 0.9300 - val_sparse_rmse: 0.9011\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8058 - sparse_rmse: 0.8335 - val_loss: 0.9277 - val_sparse_rmse: 0.8998\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.7990 - sparse_rmse: 0.8294 - val_loss: 0.9306 - val_sparse_rmse: 0.9012\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 4s 115ms/step - loss: 0.8026 - sparse_rmse: 0.8314 - val_loss: 0.9299 - val_sparse_rmse: 0.9008\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8077 - sparse_rmse: 0.8345 - val_loss: 0.9289 - val_sparse_rmse: 0.9007\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8038 - sparse_rmse: 0.8327 - val_loss: 0.9286 - val_sparse_rmse: 0.9007\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8027 - sparse_rmse: 0.8319 - val_loss: 0.9296 - val_sparse_rmse: 0.9012\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8002 - sparse_rmse: 0.8304 - val_loss: 0.9294 - val_sparse_rmse: 0.9009\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 4s 115ms/step - loss: 0.8060 - sparse_rmse: 0.8339 - val_loss: 0.9295 - val_sparse_rmse: 0.9011\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8060 - sparse_rmse: 0.8339 - val_loss: 0.9289 - val_sparse_rmse: 0.9008\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8025 - sparse_rmse: 0.8314 - val_loss: 0.9290 - val_sparse_rmse: 0.9003\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8023 - sparse_rmse: 0.8313 - val_loss: 0.9286 - val_sparse_rmse: 0.9001\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8047 - sparse_rmse: 0.8328 - val_loss: 0.9287 - val_sparse_rmse: 0.9003\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8016 - sparse_rmse: 0.8309 - val_loss: 0.9298 - val_sparse_rmse: 0.9007\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8017 - sparse_rmse: 0.8304 - val_loss: 0.9289 - val_sparse_rmse: 0.8997\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8000 - sparse_rmse: 0.8293 - val_loss: 0.9279 - val_sparse_rmse: 0.8994\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.7961 - sparse_rmse: 0.8271 - val_loss: 0.9293 - val_sparse_rmse: 0.9005\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.7968 - sparse_rmse: 0.8280 - val_loss: 0.9309 - val_sparse_rmse: 0.9015\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8031 - sparse_rmse: 0.8318 - val_loss: 0.9295 - val_sparse_rmse: 0.9007\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.7984 - sparse_rmse: 0.8288 - val_loss: 0.9306 - val_sparse_rmse: 0.9011\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 4s 115ms/step - loss: 0.8040 - sparse_rmse: 0.8324 - val_loss: 0.9288 - val_sparse_rmse: 0.9005\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8050 - sparse_rmse: 0.8328 - val_loss: 0.9293 - val_sparse_rmse: 0.9004\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8027 - sparse_rmse: 0.8313 - val_loss: 0.9304 - val_sparse_rmse: 0.9008\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8001 - sparse_rmse: 0.8296 - val_loss: 0.9293 - val_sparse_rmse: 0.9003\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.7996 - sparse_rmse: 0.8294 - val_loss: 0.9300 - val_sparse_rmse: 0.9005\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.8005 - sparse_rmse: 0.8296 - val_loss: 0.9288 - val_sparse_rmse: 0.8999\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.7993 - sparse_rmse: 0.8291 - val_loss: 0.9300 - val_sparse_rmse: 0.9006\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 4s 115ms/step - loss: 0.7985 - sparse_rmse: 0.8287 - val_loss: 0.9292 - val_sparse_rmse: 0.9004\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.7987 - sparse_rmse: 0.8287 - val_loss: 0.9295 - val_sparse_rmse: 0.9001\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 4s 115ms/step - loss: 0.8021 - sparse_rmse: 0.8306 - val_loss: 0.9292 - val_sparse_rmse: 0.9001\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.7970 - sparse_rmse: 0.8275 - val_loss: 0.9306 - val_sparse_rmse: 0.9006\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8005 - sparse_rmse: 0.8294 - val_loss: 0.9305 - val_sparse_rmse: 0.9005\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.8011 - sparse_rmse: 0.8297 - val_loss: 0.9285 - val_sparse_rmse: 0.8995\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.7980 - sparse_rmse: 0.8282 - val_loss: 0.9295 - val_sparse_rmse: 0.9000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_datagen(train_ratings, train_mask, batch_size=BATCH_SIZE),\n",
    "    validation_data=test_datagen(train_ratings, train_mask, test_ratings, test_mask, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=train_ratings.shape[0] // BATCH_SIZE,\n",
    "    validation_steps=test_ratings.shape[0] // BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      " 1/35 [..............................] - ETA: 6s - loss: 3.6028 - sparse_rmse: 1.0589WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      " 2/35 [>.............................] - ETA: 4s - loss: 3.5478 - sparse_rmse: 1.0325WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      " 3/35 [=>............................] - ETA: 3s - loss: 3.5527 - sparse_rmse: 1.0354WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "35/35 [==============================] - 4s 100ms/step - loss: 3.3559 - sparse_rmse: 0.9871 - val_loss: 3.1882 - val_sparse_rmse: 0.9734\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 3s 97ms/step - loss: 2.9747 - sparse_rmse: 0.9392 - val_loss: 2.8814 - val_sparse_rmse: 0.9628\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 3s 98ms/step - loss: 2.6592 - sparse_rmse: 0.9134 - val_loss: 2.6190 - val_sparse_rmse: 0.9551\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 4s 107ms/step - loss: 2.3975 - sparse_rmse: 0.8962 - val_loss: 2.3920 - val_sparse_rmse: 0.9480\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 3s 98ms/step - loss: 2.1804 - sparse_rmse: 0.8864 - val_loss: 2.1954 - val_sparse_rmse: 0.9416\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 3s 98ms/step - loss: 1.9823 - sparse_rmse: 0.8716 - val_loss: 2.0263 - val_sparse_rmse: 0.9363\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 4s 103ms/step - loss: 1.8244 - sparse_rmse: 0.8661 - val_loss: 1.8801 - val_sparse_rmse: 0.9316\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 4s 105ms/step - loss: 1.6865 - sparse_rmse: 0.8605 - val_loss: 1.7528 - val_sparse_rmse: 0.9271\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 4s 103ms/step - loss: 1.5633 - sparse_rmse: 0.8534 - val_loss: 1.6451 - val_sparse_rmse: 0.9247\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 4s 102ms/step - loss: 1.4605 - sparse_rmse: 0.8499 - val_loss: 1.5503 - val_sparse_rmse: 0.9218\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 4s 102ms/step - loss: 1.3723 - sparse_rmse: 0.8468 - val_loss: 1.4697 - val_sparse_rmse: 0.9199\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 4s 102ms/step - loss: 1.2939 - sparse_rmse: 0.8432 - val_loss: 1.3999 - val_sparse_rmse: 0.9186\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 4s 103ms/step - loss: 1.2331 - sparse_rmse: 0.8442 - val_loss: 1.3393 - val_sparse_rmse: 0.9170\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 4s 109ms/step - loss: 1.1717 - sparse_rmse: 0.8399 - val_loss: 1.2870 - val_sparse_rmse: 0.9159\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 4s 103ms/step - loss: 1.1182 - sparse_rmse: 0.8361 - val_loss: 1.2415 - val_sparse_rmse: 0.9147\n",
      "Epoch 16/100\n",
      "21/35 [=================>............] - ETA: 1s - loss: 1.0741 - sparse_rmse: 0.8293"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1602e0f94401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/envs/dp/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/miniconda3/envs/dp/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dp/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/miniconda3/envs/dp/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dp/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dp/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[0;32m~/miniconda3/envs/dp/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;31m# Add regularization losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0mcustom_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_losses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       total_loss += losses_utils.scale_loss_for_distribution(\n",
      "\u001b[0;32m~/miniconda3/envs/dp/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mlosses\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mloss_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0mcollected_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollected_losses\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_subclass_implementers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dp/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_layers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m       nested_layers = trackable_layer_utils.filter_empty_layer_containers(\n\u001b[0;32m-> 2333\u001b[0;31m           self._layers)\n\u001b[0m\u001b[1;32m   2334\u001b[0m       return list(\n\u001b[1;32m   2335\u001b[0m           itertools.chain.from_iterable(\n",
      "\u001b[0;32m~/miniconda3/envs/dp/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/layer_utils.py\u001b[0m in \u001b[0;36mfilter_empty_layer_containers\u001b[0;34m(layer_list)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mexisting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dp/lib/python3.7/site-packages/tensorflow_core/python/util/object_identity.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_datagen(train_ratings, train_mask, batch_size=BATCH_SIZE),\n",
    "    validation_data=test_datagen(train_ratings, train_mask, test_ratings, test_mask, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=train_ratings.shape[0] // BATCH_SIZE,\n",
    "    validation_steps=test_ratings.shape[0] // BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
